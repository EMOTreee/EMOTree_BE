import os
import json
import numpy as np
from pathlib import Path
from typing import Dict, Optional
import torch
import soundfile as sf
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnableMap
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferWindowMemory

from transformers import AutoModelForAudioClassification, Wav2Vec2FeatureExtractor

from preprocessing.audio_preprocessing import preprocess_audio
from app.utils.acoustic_features import (
    extract_all_features,
    calculate_z_scores_with_interpretation,
    format_acoustic_features_for_llm
)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent.parent

# Baseline JSON ë¡œë“œ
BASELINE_PATH = PROJECT_ROOT / "document" / "emotion_baseline.json"
with open(BASELINE_PATH, "r", encoding="utf-8") as f:
    EMOTION_BASELINE = json.load(f)

# Wav2Vec2 ê°ì • ì¸ì‹ ëª¨ë¸ ë¡œë“œ
MODEL_PATH = PROJECT_ROOT / "models" / "ehcalabres_emotion"
emotion_model = AutoModelForAudioClassification.from_pretrained(str(MODEL_PATH))
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(str(MODEL_PATH))

# ê°ì • ë¼ë²¨ ë§¤í•‘
EMOTION_MAPPING = {
    "angry": "ANGER",
    "happy": "JOY",
    "sad": "SADNESS",
    "surprise": "SURPRISE",
    "fear": "ANXIETY",
    "neutral": "NEUTRAL",
    "disgust": "ANGER"
}

# ê°ì • ë¼ë²¨ í•œê¸€ ë§¤í•‘ (í”¼ë“œë°±ìš©)
EMOTION_KOREAN = {
    "ANGER": "ë¶„ë…¸",
    "JOY": "ê¸°ì¨",
    "SADNESS": "ìŠ¬í””",
    "SURPRISE": "ë†€ëŒ",
    "ANXIETY": "ë¶ˆì•ˆ",
    "NEUTRAL": "ì¤‘ë¦½"
}

# LLM ëª¨ë¸ ì´ˆê¸°í™”
chat_openai = ChatOpenAI(
    temperature=0.7,
    max_tokens=600,
    model="gpt-4o-mini",
)

# ì‚¬ìš©ìë³„ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ (ì¸ë©”ëª¨ë¦¬)
user_memories: Dict[int, ConversationBufferWindowMemory] = {}

# ë©”ëª¨ë¦¬ ê´€ë¦¬ í•¨ìˆ˜
def get_or_create_memory(user_id: int) -> ConversationBufferWindowMemory:
    if user_id not in user_memories:
        user_memories[user_id] = ConversationBufferWindowMemory(
            k=5,
            input_key="user_attempt",
            output_key="feedback_result",
            memory_key="chat_history",
            return_messages=False
        )
        print(f"[INFO] ì‚¬ìš©ì {user_id}ì˜ ìƒˆ ë©”ëª¨ë¦¬ ìƒì„±")
    return user_memories[user_id]

# ë©”ëª¨ë¦¬ ì´ˆê¸°í™” í•¨ìˆ˜
def reset_memory(user_id: int) -> None:
    if user_id in user_memories:
        user_memories[user_id].clear()
        print(f"[INFO] ì‚¬ìš©ì {user_id}ì˜ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”")
    else:
        print(f"[INFO] ì‚¬ìš©ì {user_id}ì˜ ë©”ëª¨ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ")

# 1. ì „ì²˜ë¦¬ ì²´ì¸
def preprocess_audio_chain(audio_bytes: bytes) -> np.ndarray:
    try:
        preprocessed = preprocess_audio(audio_bytes, sr=16000)
        return preprocessed
    except Exception as e:
        print(f"[ERROR] ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise

# 2-1. ê°ì • ê°ì§€ ì²´ì¸
def detect_emotion_chain(audio_array: np.ndarray) -> Dict[str, any]:
    try:
        # íŠ¹ì§• ì¶”ì¶œ
        inputs = feature_extractor(
            audio_array,
            sampling_rate=16000,
            return_tensors="pt",
            padding=True
        )
        
        # ëª¨ë¸ ì¶”ë¡ 
        with torch.no_grad():
            logits = emotion_model(**inputs).logits
        
        # í™•ë¥ ê°’ ê³„ì‚°
        probs = torch.softmax(logits, dim=-1)[0]
        
        # ìƒìœ„ 3ê°œ ê°ì • ê°€ì ¸ì˜¤ê¸°
        top_k = torch.topk(probs, k=min(3, len(probs)))
        top_indices = top_k.indices.tolist()
        top_probs = top_k.values.tolist()
        
        # ìƒìœ„ 3ê°œ ê²°ê³¼ êµ¬ì„±
        top3_results = []
        for idx, prob in zip(top_indices, top_probs):
            original_label = emotion_model.config.id2label[idx].lower()
            mapped_emotion = EMOTION_MAPPING.get(original_label, "NEUTRAL")
            top3_results.append({
                "emotion": mapped_emotion,
                "confidence": round(prob * 100, 2)  # ë°±ë¶„ìœ¨ë¡œ ë³€í™˜
            })
        
        # 1ìœ„ ê°ì •
        primary_emotion = top3_results[0]["emotion"]
        
        print(f"[INFO] ê°ì • ê°ì§€ ê²°ê³¼:")
        for i, result in enumerate(top3_results, 1):
            print(f"  {i}ìœ„: {result['emotion']} ({result['confidence']}%)")
        
        # top3ë¥¼ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
        top3_formatted = "\n".join([
            f"{i}ìœ„: {item['emotion']} ({item['confidence']}%)"
            for i, item in enumerate(top3_results, 1)
        ])
        
        return {
            "primary": primary_emotion,
            "top3": top3_results,
            "top3_formatted": top3_formatted
        }
        
    except Exception as e:
        print(f"[ERROR] ê°ì • ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            "primary": "NEUTRAL",
            "top3": [{"emotion": "NEUTRAL", "confidence": 0.0}],
            "top3_formatted": "1ìœ„: NEUTRAL (0.0%)"
        }

# 2-2-1. STT ì²´ì¸
def perform_stt(audio_array: np.ndarray) -> Optional[str]:
    try:
        import tempfile
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
            sf.write(tmp_file.name, audio_array, 16000, format='WAV')
            audio_file_path = tmp_file.name
        
        try:
            from openai import OpenAI
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            with open(audio_file_path, "rb") as audio_file:
                transcription = client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file
                )
            text = transcription.text
            print(f"[INFO] STT ê²°ê³¼: {text}")
            return text
            
        finally:
            import os as os_module
            if os_module.path.exists(audio_file_path):
                os_module.unlink(audio_file_path)
        
    except Exception as e:
        print(f"[ERROR] STT ì‹¤íŒ¨: {e}")
        return None

# 2-2-2. í…ìŠ¤íŠ¸ í‰ê°€ ì²´ì¸
def build_text_evaluation_chain():
    evaluation_prompt = ChatPromptTemplate.from_template("""
            ë‹¹ì‹ ì€ ìŒì„± ê°ì • í‘œí˜„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

            ì‚¬ìš©ìê°€ "{target_emotion}" ê°ì •ì„ í‘œí˜„í•˜ë ¤ê³  ìŒì„±ìœ¼ë¡œ ë§í•œ ë‚´ìš©ì´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
            "{text}"

            ì´ í…ìŠ¤íŠ¸ê°€ ëª©í‘œ ê°ì •ì¸ "{target_emotion}"ì„ í‘œí˜„í•˜ê¸°ì— ì ì ˆí•œì§€ í‰ê°€í•´ì£¼ì„¸ìš”.
            í‰ê°€ ë‚´ìš©ì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            - ë‹¨ì–´ ì„ íƒì˜ ì ì ˆì„±
            - ê°ì • í‘œí˜„ì˜ ëª…í™•ì„±
            - ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„

            í‰ê°€ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
""")
    
    return evaluation_prompt | chat_openai | StrOutputParser()


# 2-3. ìŒí–¥ íŠ¹ì§• ì¶”ì¶œ ì²´ì¸
def extract_acoustic_features(audio_array: np.ndarray, target_emotion: str) -> Optional[str]:
    try:
        # ìŒí–¥ íŠ¹ì§• ì¶”ì¶œ
        features = extract_all_features(audio_array)
        if features is None:
            return None
        
        # Target emotionì˜ baseline ê°€ì ¸ì˜¤ê¸°
        target_baseline = EMOTION_BASELINE.get(target_emotion)
        if not target_baseline:
            print(f"[WARN] {target_emotion}ì˜ baseline ì—†ìŒ")
            return None
        
        # Z-score ê³„ì‚° ë° ìì—°ì–´ í•´ì„
        z_scores = calculate_z_scores_with_interpretation(
            features,
            target_baseline["mean"],
            target_baseline["std"]
        )
        
        # ì „ì²´ íŠ¹ì§•ì„ êµ¬ì¡°í™”í•˜ì—¬ í¬ë§·
        formatted_features = format_acoustic_features_for_llm(z_scores)
        
        analysis_text = f"""
            [ëª©í‘œ ê°ì • '{target_emotion}'ê³¼ì˜ ìŒí–¥ íŠ¹ì§• ë¹„êµ]

            {formatted_features}
        """
        
        print(f"[INFO] ìŒí–¥ íŠ¹ì§• ë¶„ì„ ì™„ë£Œ")
        return analysis_text
        
    except Exception as e:
        print(f"[ERROR] ìŒí–¥ íŠ¹ì§• ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


# 3. ì¢…í•© í‰ê°€ ë° í”¼ë“œë°± ìƒì„± ì²´ì¸
def build_feedback_generation_chain():
    prompt_template = """
ë‹¹ì‹ ì€ ë”°ëœ»í•˜ê³  ì¹œì ˆí•œ ìŒì„± ê°ì • í‘œí˜„ ì½”ì¹˜ì…ë‹ˆë‹¤.

### [ì—­í•  ë° ì²­ì¤‘]
1. **ì²­ì¤‘:** ë‹¹ì‹ ì˜ í”¼ë“œë°±ì€ ìŒì„± í›ˆë ¨ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ” ì¼ë°˜ì¸ì„ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.
2. **í†¤ì•¤ë§¤ë„ˆ:** ê¸ì •ì ì´ê³  ê±´ì„¤ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ë©°, ì˜í•œ ì ê³¼ ê°œì„ í•  ì ì„ ê· í˜• ìˆê²Œ ì œì‹œí•˜ì‹­ì‹œì˜¤.

### [í•µì‹¬ ì§€ì¹¨: ê¸°ìˆ  ìš©ì–´ ì‚¬ìš© ê¸ˆì§€] ğŸš¨
ë‹¤ìŒê³¼ ê°™ì€ ì „ë¬¸ ìŒí–¥ íŠ¹ì§• ìš©ì–´ë¥¼ **ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤:**
- mfcc, rms, f0, spectral centroid, z-score, pitch, energy, speaking_rate, pause_ratio ë“±

ëª¨ë“  ë¶„ì„ ë‚´ìš©ì€ ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì–¸ì–´ë¡œ í•´ì„í•˜ì—¬ ì„¤ëª…í•´ì•¼ í•©ë‹ˆë‹¤.

### [ê¸°ìˆ  ìš©ì–´ í•´ì„ ê°€ì´ë“œ]
ë¶„ì„ ê²°ê³¼ì— ë‚˜íƒ€ë‚˜ëŠ” ê¸°ìˆ  ìš©ì–´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í•´ì„í•˜ì—¬ ì¼ë°˜ ì–¸ì–´ë¡œ ì¹˜í™˜í•˜ì‹­ì‹œì˜¤:
- **Pitch (f0, mean_f0, std_f0, range_f0):** "ëª©ì†Œë¦¬ í†¤ì˜ ë†’ë‚®ì´", "ëª©ì†Œë¦¬ê°€ ì–¼ë§ˆë‚˜ ë†’ì€ì§€/ë‚®ì€ì§€", "ëª©ì†Œë¦¬ í†¤ì˜ ë³€í™”"
- **Energy (rms, mean_rms, std_rms, max_rms):** "ëª©ì†Œë¦¬ì˜ í¬ê¸°", "ëª©ì†Œë¦¬ì— ì‹¤ë¦° í˜", "ëª©ì†Œë¦¬ì˜ ê°•ì•½"
- **Rate & Pause (speaking_rate, pause_ratio, mean_voiced_segment_length):** "ë§ì˜ ë¹ ë¥´ê¸°", "ë§í•˜ëŠ” ì†ë„", "ë§ ì‚¬ì´ì˜ ì‰¼", "ë§ì˜ íë¦„"
- **Timbre (mfcc, spectral_centroid):** "ëª©ì†Œë¦¬ì˜ ìš¸ë¦¼", "ëª©ì†Œë¦¬ ìƒ‰ê¹”", "ëª©ì†Œë¦¬ì˜ ë°ê¸°/ì–´ë‘¡ê¸°", "ëª©ì†Œë¦¬ì— ë‹´ê¸´ ë–¨ë¦¼"

### [ê°ì • ë¼ë²¨ í•œê¸€ í‘œê¸°]
í”¼ë“œë°± ì‘ì„± ì‹œ ê°ì • ë¼ë²¨ì€ ë°˜ë“œì‹œ ë‹¤ìŒê³¼ ê°™ì´ í•œê¸€ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤:
- ANGER â†’ "ë¶„ë…¸"
- JOY â†’ "ê¸°ì¨"
- SADNESS â†’ "ìŠ¬í””"
- SURPRISE â†’ "ë†€ëŒ"
- ANXIETY â†’ "ë¶ˆì•ˆ"
- NEUTRAL â†’ "ì¤‘ë¦½"

ì‚¬ìš©ìê°€ "{target_emotion_korean}" ê°ì •ì„ ìŒì„±ìœ¼ë¡œ í‘œí˜„í–ˆìŠµë‹ˆë‹¤.

[ì´ì „ ì‹œë„ ì´ë ¥]
{chat_history}

[ê°ì • ì¸ì‹ ëª¨ë¸ ë¶„ì„ ê²°ê³¼ (ìƒìœ„ 3ê°œ)]
{detected_emotion_top3}

[ìŒí–¥ íŠ¹ì§• ë¶„ì„]
{acoustic_analysis}

[í…ìŠ¤íŠ¸ í‰ê°€]
{text_evaluation}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

1. **ìµœì¢… ê°ì • íŒë‹¨**: 
   - ê°ì • ì¸ì‹ ëª¨ë¸ ê²°ê³¼ì™€ ìŒí–¥ íŠ¹ì§• ë¶„ì„ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ í‘œí˜„í•œ ê°ì •ì„ íŒë‹¨í•˜ì„¸ìš”
   - ëª¨ë¸ì˜ 1ìœ„ ê²°ê³¼ê°€ ìŒí–¥ íŠ¹ì§•ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ìŒí–¥ íŠ¹ì§•ì„ ë” ìš°ì„ ì‹œí•˜ì„¸ìš”
   - íŒë‹¨í•œ ê°ì •ì€ ë°˜ë“œì‹œ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤: JOY, SADNESS, ANGER, SURPRISE, ANXIETY, NEUTRAL
   - NEUTRALì€ ì‚¬ìš©ìê°€ íŠ¹ì • ê°ì •ì„ ëª…í™•íˆ í‘œí˜„í•˜ì§€ ëª»í•œ ì¼ë¶€ ê²½ìš°ì—ë§Œ ì„ íƒí•˜ê³  ê°€ëŠ¥í•˜ë©´ í”¼í•˜ì„¸ìš”

2. **ì ìˆ˜ ì‚°ì •** (0~100):
   - íŒë‹¨ëœ ê°ì •ì´ ëª©í‘œ ê°ì •ê³¼ ì¼ì¹˜: ê°€ì¥ ì¤‘ìš” (ì•½ 40%)
   - ìŒí–¥ íŠ¹ì§•ì´ ëª©í‘œ ê°ì •ì˜ baselineê³¼ ìœ ì‚¬: ì¤‘ìš” (ì•½ 40%)
   - í…ìŠ¤íŠ¸ ì ì ˆì„±: ì¤‘ìš” (ì•½ 20%)

3. **ìƒì„¸ í”¼ë“œë°± ì‘ì„± (ì¼ë°˜ì¸ ì¹œí™”ì  ì–¸ì–´ í•„ìˆ˜):**
   - ì˜í•œ ì ê³¼ ê°œì„ í•  ì ì„ êµ¬ì²´ì ì´ì§€ë§Œ ì‰¬ìš´ ë§ë¡œ ì œì‹œ
   - ê¸°ìˆ  ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ê³ , ìœ„ í•´ì„ ê°€ì´ë“œë¥¼ ì°¸ê³ í•˜ì—¬ ì¼ë°˜ ì–¸ì–´ë¡œ ì„¤ëª…
   - ì˜ˆ: "ëª©ì†Œë¦¬ í†¤ì´ ì ì ˆí–ˆì–´ìš”", "ë§ì˜ ë¹ ë¥´ê¸°ë¥¼ ì¡°ì ˆí•´ë³´ì„¸ìš”", "ëª©ì†Œë¦¬ì— í˜ì„ ë” ì‹¤ì–´ë³´ì„¸ìš”"
   - **ê°ì • ë¼ë²¨ì€ ë°˜ë“œì‹œ í•œê¸€ë¡œ í‘œê¸°í•˜ì„¸ìš”** (ì˜ˆ: "ìŠ¬í””", "ë¶ˆì•ˆ", "ê¸°ì¨" ë“±)
   - **ì´ì „ ì‹œë„ ì´ë ¥ì´ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ì´ë²ˆ ì‹œë„ì˜ ì ìˆ˜ê°€ ìƒìŠ¹/í•˜ë½í–ˆëŠ”ì§€, ì–´ë–¤ ë¶€ë¶„(ëª©ì†Œë¦¬ í†¤, í¬ê¸°, ë§ì˜ ë¹ ë¥´ê¸° ë“±)ì´ ê°œì„ ë˜ì—ˆê±°ë‚˜ ì•…í™”ë˜ì—ˆëŠ”ì§€ ì‰½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”**

ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "detectedEmotion": "íŒë‹¨ëœ ê°ì • (JOY, SADNESS, ANGER, SURPRISE, ANXIETY, NEUTRAL ì¤‘ í•˜ë‚˜)",
    "score": 0~100 ì‚¬ì´ì˜ ì •ìˆ˜,
    "feedback": "ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ìƒì„¸í•œ í”¼ë“œë°± (ê¸°ìˆ  ìš©ì–´ ì—†ì´ ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì–¸ì–´ë¡œ, ê°ì • ë¼ë²¨ì€ í•œê¸€ë¡œ)"
}}

ë§Œì•½ í…ìŠ¤íŠ¸ í‰ê°€ë‚˜ ìŒí–¥ íŠ¹ì§• ë¶„ì„ì´ "ë¶„ì„ ë¶ˆê°€"ì¸ ê²½ìš°, í•´ë‹¹ ë¶€ë¶„ì„ ì œì™¸í•˜ê³  í‰ê°€í•´ì£¼ì„¸ìš”.
ì´ì „ ì‹œë„ ì´ë ¥ì´ ë¹„ì–´ìˆë‹¤ë©´("ì´ì „ ì‹œë„ ì´ë ¥: ì—†ìŒ" ë˜ëŠ” ë¹ˆ ë¬¸ìì—´) ì²« ì‹œë„ì´ë¯€ë¡œ ì´ë ¥ ê´€ë ¨ ì–¸ê¸‰ ì—†ì´ í˜„ì¬ ì‹œë„ë§Œ í‰ê°€í•˜ì„¸ìš”.

### [í”¼ë“œë°± ì‘ì„± ì˜ˆì‹œ]

**ì˜ˆì‹œ 1: ëª©í‘œ ê°ì • ë‹¬ì„±**
{{
    "detectedEmotion": "JOY",
    "score": 95,
    "feedback": "ëŒ€ë‹¨í•©ë‹ˆë‹¤! ì´ë²ˆ ì‹œë„ì—ì„œ ëª©í‘œ ê°ì •ì¸ 'ê¸°ì¨'ì„ ì™„ë²½í•˜ê²Œ í‘œí˜„í•˜ì…¨ìŠµë‹ˆë‹¤. ì´ì „ ê¸°ë¡ì—ì„œ ëª©ì†Œë¦¬ í†¤ì´ ë‹¤ì†Œ ë‚®ê²Œ ìœ ì§€ë˜ì–´ ì•„ì‰¬ì› ëŠ”ë°, ì´ë²ˆì—ëŠ” ëª©ì†Œë¦¬ í†¤ì˜ ë†’ë‚®ì´ ë³€í™” í­ì´ í¬ê²Œ ëŠê»´ì ¸ ë§¤ìš° ìƒë™ê°ì´ ë„˜ì¹©ë‹ˆë‹¤. ëª©ì†Œë¦¬ í¬ê¸°ë„ ì¼ê´€ë˜ê²Œ ìœ ì§€ë˜ì—ˆê³ , ëª©ì†Œë¦¬ ìš¸ë¦¼ì´ ë°ê³  ì„ ëª…í•´ì¡Œì–´ìš”. í…ìŠ¤íŠ¸ ë‚´ìš©ë„ ì ì ˆí–ˆì§€ë§Œ, ë¬´ì—‡ë³´ë‹¤ ìŒì„±ì  íŠ¹ì§•ì´ ê¸°ì¤€ì ì„ ë„˜ì–´ì„°ìŠµë‹ˆë‹¤. ì•„ì£¼ í›Œë¥­í•´ìš”! ë‹¤ìŒ ê°ì •ìœ¼ë¡œ ë„˜ì–´ê°€ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤."
}}

**ì˜ˆì‹œ 2: ê°œì„  í•„ìš”**
{{
    "detectedEmotion": "NEUTRAL",
    "score": 58,
    "feedback": "ì´ë²ˆ ì‹œë„ì—ì„œëŠ” ìŠ¬í””ë³´ë‹¤ëŠ” í‰ì´í•˜ê³  í˜ì—†ëŠ” ëª©ì†Œë¦¬ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ëŠ” ìŠ¬í””ì„ í‘œí˜„í•˜ê¸° ì¢‹ì•˜ì§€ë§Œ, ë§ì˜ ë¹ ë¥´ê¸°ê°€ ë„ˆë¬´ ë¹¨ë¼ì„œ ì§„ì§€í•¨ì´ ë¶€ì¡±í–ˆê³ , ëª©ì†Œë¦¬ í¬ê¸°ê°€ ë„ˆë¬´ ë‚®ì•„ ê°ì •ì´ ì „ë‹¬ë˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¬í””ì„ í‘œí˜„í•˜ë ¤ë©´ ë§ì˜ ë¹ ë¥´ê¸°ë¥¼ í›¨ì”¬ ë” ëŠë¦¬ê²Œ ì¡°ì ˆí•˜ê³ , ë§ ì‚¬ì´ì— ì‰¼ì„ ì˜ë„ì ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì„¸ìš”. ë˜í•œ, ëª©ì†Œë¦¬ í†¤ì„ í‰ê· ë³´ë‹¤ ë‚®ê²Œ ìœ ì§€í•˜ëŠ” ê²ƒì— ì§‘ì¤‘í•˜ì—¬ ê°ì •ì˜ ê¹Šì´ë¥¼ ë”í•˜ë©´ ë‹¤ìŒì—ëŠ” ì ìˆ˜ê°€ í¬ê²Œ ì˜¤ë¥¼ ê²ƒì…ë‹ˆë‹¤. ì•„ì§ì€ ì—°ìŠµì´ ë” í•„ìš”í•©ë‹ˆë‹¤."
}}

ìœ„ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬, ê¸ì •ì ì´ê³  êµ¬ì²´ì ì´ë©° ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ ì‘ì„±í•˜ì„¸ìš”.
JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. ì½”ë“œ ë¸”ë¡ì´ë‚˜ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
"""
    
    prompt = ChatPromptTemplate.from_template(prompt_template)
    base_chain = prompt | chat_openai | JsonOutputParser()
    
    # ë©”ëª¨ë¦¬ í†µí•© ë˜í¼ í•¨ìˆ˜
    def create_chain_with_memory(memory: Optional[ConversationBufferWindowMemory]):
        def chain_with_memory(inputs: Dict) -> Dict:
            # ë©”ëª¨ë¦¬ì—ì„œ ì´ì „ ëŒ€í™” ë¡œë“œ
            chat_history = ""
            if memory:
                memory_vars = memory.load_memory_variables({})
                chat_history = memory_vars.get("chat_history", "")
                if not chat_history:
                    chat_history = "ì´ì „ ì‹œë„ ì´ë ¥: ì—†ìŒ"
            else:
                chat_history = "ì´ì „ ì‹œë„ ì´ë ¥: ì—†ìŒ"
            
            # í”„ë¡¬í”„íŠ¸ì— chat_history ì¶”ê°€
            inputs_with_history = {**inputs, "chat_history": chat_history}
            
            # ì²´ì¸ ì‹¤í–‰
            result = base_chain.invoke(inputs_with_history)
            
            # ë©”ëª¨ë¦¬ì— í˜„ì¬ ì‹œë„ ì €ì¥
            if memory:
                user_attempt = f"ëª©í‘œ ê°ì •: {inputs['target_emotion']}, ê°ì§€ëœ ê°ì •: {result.get('detectedEmotion', 'NEUTRAL')}, ì ìˆ˜: {result.get('score', 0)}"
                feedback_result = result.get('feedback', '')
                memory.save_context(
                    {"user_attempt": user_attempt},
                    {"feedback_result": feedback_result}
                )
            
            return result
        
        return chain_with_memory
    
    return create_chain_with_memory

# ì „ì²´ íŒŒì´í”„ë¼ì¸
def build_voice_emotion_pipeline():
    
    # í…ìŠ¤íŠ¸ í‰ê°€ ì²´ì¸
    text_evaluation_chain = build_text_evaluation_chain()
    
    # ì¢…í•© í‰ê°€ ë° í”¼ë“œë°± ìƒì„± ì²´ì¸ íŒ©í† ë¦¬
    feedback_chain_factory = build_feedback_generation_chain()
    
    # 1ë‹¨ê³„: ì „ì²˜ë¦¬ ì²´ì¸
    preprocess_chain = RunnableLambda(
        lambda x: {
            "audio_array": preprocess_audio_chain(x["audio_bytes"]),
            "target_emotion": x["target_emotion"]
        }
    )
    
    # 2-2: STT + í…ìŠ¤íŠ¸ í‰ê°€ ì²´ì¸ (ìˆœì°¨ ì‹¤í–‰)
    def stt_and_text_evaluation(x):
        # STT ì‹¤í–‰
        transcribed_text = perform_stt(x["audio_array"]) or ""
        
        # í…ìŠ¤íŠ¸ í‰ê°€ (STT ê²°ê³¼ ì‚¬ìš©)
        text_eval = text_evaluation_chain.invoke({
            "target_emotion": x["target_emotion"],
            "text": transcribed_text
        }) if transcribed_text else "ë¶„ì„ ë¶ˆê°€"
        
        return text_eval
    
    # 2ë‹¨ê³„: ë³‘ë ¬ ë¶„ì„ ì²´ì¸ (ê°ì • ê°ì§€ + STTâ†’í…ìŠ¤íŠ¸í‰ê°€ + ìŒí–¥ íŠ¹ì§•)
    parallel_analysis_chain = RunnableMap({
        "detected_emotion": RunnableLambda(
            lambda x: detect_emotion_chain(x["audio_array"])
        ),
        "text_evaluation": RunnableLambda(stt_and_text_evaluation),
        "acoustic_analysis": RunnableLambda(
            lambda x: extract_acoustic_features(x["audio_array"], x["target_emotion"]) or "ë¶„ì„ ë¶ˆê°€"
        ),
        "target_emotion": lambda x: x["target_emotion"]
    })
    
    def pipeline(audio_bytes: bytes, target_emotion: str, user_id: int, reset_flag: bool = False) -> Dict:
        try:
            print(f"[PIPELINE] ì‚¬ìš©ì {user_id} - ì „ì²˜ë¦¬|ë³‘ë ¬ë¶„ì„|í”¼ë“œë°± ì²´ì¸ ì‹¤í–‰ ì‹œì‘...")
            
            # ë©”ëª¨ë¦¬ ì´ˆê¸°í™” ì²˜ë¦¬
            if reset_flag:
                reset_memory(user_id)
            
            # ì‚¬ìš©ì ë©”ëª¨ë¦¬ ê°€ì ¸ì˜¤ê¸°
            memory = get_or_create_memory(user_id)
            
            # 3ë‹¨ê³„: í”¼ë“œë°± ìƒì„± ì²´ì¸ (ë©”ëª¨ë¦¬ ì£¼ì…)
            def feedback_step(analysis_result: Dict) -> Dict:
                target_emotion_korean = EMOTION_KOREAN.get(analysis_result["target_emotion"], analysis_result["target_emotion"])
                
                feedback_input = {
                    "target_emotion": analysis_result["target_emotion"],
                    "target_emotion_korean": target_emotion_korean,
                    "detected_emotion_top3": analysis_result["detected_emotion"]["top3_formatted"],
                    "text_evaluation": analysis_result["text_evaluation"],
                    "acoustic_analysis": analysis_result["acoustic_analysis"]
                }
                
                feedback_chain = feedback_chain_factory(memory)
                return feedback_chain(feedback_input)
            
            # 4ë‹¨ê³„: ìµœì¢… ê²°ê³¼ í¬ë§·íŒ…
            def format_result(x: Dict) -> Dict:
                analysis_result = x["analysis"]
                feedback_result = x["feedback"]
                
                detected_emotion = feedback_result.get("detectedEmotion", "NEUTRAL")
                is_correct = (detected_emotion == x["target_emotion"])
                
                result = {
                    "targetEmotion": x["target_emotion"],
                    "detectedEmotion": detected_emotion,
                    "detectedEmotionTop3": analysis_result["detected_emotion"]["top3"],
                    "score": feedback_result.get("score", 50),
                    "feedback": feedback_result.get("feedback", "í‰ê°€ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤."),
                    "isCorrect": is_correct
                }
                
                # ì •ë‹µ ë‹¬ì„± ì‹œ ë©”ëª¨ë¦¬ ìë™ ì´ˆê¸°í™”
                if is_correct:
                    print(f"[SUCCESS] ì‚¬ìš©ì {user_id} - ëª©í‘œ ê°ì • ë‹¬ì„±! ë©”ëª¨ë¦¬ ì´ˆê¸°í™”")
                    reset_memory(user_id)
                
                return result
            
            # ì „ì²´ ì²´ì¸ ì—°ê²°: 1ë‹¨ê³„|2ë‹¨ê³„|3ë‹¨ê³„|4ë‹¨ê³„
            full_chain = (
                preprocess_chain 
                | parallel_analysis_chain 
                | RunnableLambda(lambda analysis: {
                    "analysis": analysis,
                    "feedback": feedback_step(analysis),
                    "target_emotion": analysis["target_emotion"]
                })
                | RunnableLambda(format_result)
            )
            
            # ì²´ì¸ ì‹¤í–‰
            result = full_chain.invoke({
                "audio_bytes": audio_bytes,
                "target_emotion": target_emotion
            })
            
            print(f"[SUCCESS] ì‚¬ìš©ì {user_id} - íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì •ë‹µ: {result['isCorrect']})")
            return result
            
        except Exception as e:
            print(f"[ERROR] íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            # ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ìƒìœ„ ë ˆì´ì–´ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
            raise
    
    return pipeline

# íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
voice_emotion_pipeline = build_voice_emotion_pipeline()
